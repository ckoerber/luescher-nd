% !TEX encoding = UTF-8 Unicode
% !TEX spellcheck = en-US
\documentclass[
    aps,
    prl,
    showkeys,
    nofootinbib,
    %twocolumn,
    floatfix
]{revtex4-1}

\usepackage[utf8]{inputenc} % UTF-8
\usepackage{braket}

\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{graphicx}

\renewcommand{\vec}[1]{\boldsymbol{#1}}


\newcommand{\fm}{\,\mathrm{fm}}
\newcommand{\ifm}{\,\mathrm{fm}^{-1}}
\newcommand{\MeV}{\,\mathrm{MeV}}

\begin{document}

\title{Discretization}

\author{C.~Körber}

\date{\today}

\begin{abstract}%
	How does the contact interaction depend on discretization effects?
\end{abstract}

\maketitle

\section{No discretization}
\subsection{1-d}
\subsubsection{Positive energies}

The momentum space Schrödinger equation
\begin{equation}
	\frac{p^2}{2 \mu} \psi(p) + \int \frac{d p}{2\pi} \braket{p | \hat V | p'} \psi(p') = E \psi(p')
\end{equation}
for contact interactions this becomes
\begin{equation}
	\frac{p^2}{2 \mu} \psi(p) +  c I_0 = E \psi(p') \, , \qquad I_0 = \int \frac{d p'}{2\pi} \psi(p') \, .
\end{equation}
Thus
\begin{equation}
	\psi(p) = \frac{c I_0}{E - \frac{p^2}{2\mu}}
\end{equation}
For scattering solutions we need $i\epsilon$ prescription
\begin{align}
	I_0 &= \int \frac{d p}{2\pi}  \frac{c I_0}{E - \frac{p^2}{2\mu} + i \epsilon} \\
	&= I_0 \times \frac{\mu c }{\pi} \int\limits_{ - \infty}^{\infty} \frac{d p}{\gamma^2 - {p^2} + i \tilde\epsilon} \, , \quad \gamma^2 = 2 \mu E > 0\,.
\end{align}
The remaining integral can be solved by the residual theorem: the zeros of the denominator are
\begin{align}
	\gamma^2 - {p^2} + i \tilde\epsilon &= (\gamma + i \tilde\epsilon - p)(\gamma + i \tilde\epsilon + p) \, .
\end{align}
Note that the zeros are independent on the sign of $\epsilon$ (one root in upper and one root in lower plane).
Completing the integral in the upper plane selects the residual $p_0 = \gamma + i \tilde \epsilon$ and thus
\begin{equation}
	\int\limits_{ - \infty}^{\infty} \frac{d p}{\gamma^2 - {p^2} + i \tilde\epsilon}
	=
	 \frac{2 \pi i}{2 \gamma} \, .
\end{equation}
and therefore
\begin{equation}
	1 = \frac{\mu c}{\pi} \times \frac{\pi i}{\gamma} \, ,
\end{equation}
or
\begin{equation}
	\gamma = i \mu c \, \Rightarrow  E = - \frac{\mu c^2}{2}
\end{equation}
In other words, you do not get positive energies for $c \in \mathbb{R}$.

It seems like, you only get positive energies once you are in a box...

\clearpage
\section{Understanding the unitary limit}


From my understanding, the unitary limit appears when the total cross section is independent of microscopic parameters.
The cross section is given by
\begin{equation}
	\frac{d \sigma}{d \Omega} = | f(\Omega, p) | ^ 2 \, ,
\end{equation}
where
\begin{align}
	f(\Omega, p) &= \sum_{l=0}^{\infty} f_l(\Omega, p) \, , \\ 
	f_l(\Omega, p) &\sim g_l(\Omega) \times \left[\exp( 2 i \delta_l (p)) - 1\right]
\end{align}
This is generally true for all $d = 1, 2, 3$ cases modulo not important factors.

In our system, for a simple contact interaction of strength $c_0$, we only find contributions to $l=0$.
Regarding your eq. (7), one finds that
\begin{equation}
	\cot \delta_0 (p) = \begin{cases}
		a_0 p &, \, d = 1 \\
		2 / \pi \log(a_0 p) &, \, d = 2 \\
		- 1/(a_0 p) &, \, d = 3
	\end{cases}
\end{equation}

In other words, we are at the unitary limit if $\cot \delta_0$ is independent of $a_0$.
This can only happen if $a_0 \rightarrow \infty$ or $a_0 \rightarrow \pm 0$ because $a_0$ must be independent of $p$.

\textbf{Does this make sense?}

To discriminate between infinite volume and lattice, I will label the momentum points which are extracted from lattice (as in continuum but finite volume) as $\gamma_i$.
Here, $\gamma_i^2 = 2 \mu E_i$, $\mu$ being the reduced two-particle mass and $E_i$ being the lattice energy spectrum.

Using your equations (35-37) and combining this with your eq. (7), one finds
\begin{equation}\label{def:a0}
	a_0 = \begin{cases}
		\frac{L}{2\pi^2}S_1(x_i) & \, , d =1 \\
		\frac{L}{2\pi} \exp \left\{ S_2(x_i) / (2\pi) \right\} & \, , d =2 \\
		\frac{ \pi L}{ S_3(x_i) } & \, , d =3
	\end{cases}
	\qquad \forall_i \, ,
\end{equation}
with $S_j(x_i)$ being the Lüscher Zeta function in $j$ dimensions and $ x_i = \mu E_i L ^2 / (2 \pi^2) $.
In other words, if your computation does the right thing, you fix your contact interaction strength $c_0$, which will give you a set of energy levels $E_i(c_0)$.
For all of the eigenstates $E_i$, all points $S_j(x_i)$ lay on a line because, because $a_0$ is independent of the energy level and eq.~\eqref{def:a0} tells us that there is a one-to-one relation between $S_j(x_i)$ and $a_0$.

So in which cases do we find that $a_0 \to 0, \pm \infty$?

\begin{itemize}
	\item For $d = 1$ find $c_0$ such that $S_1(x_i(c_0)) \to 0, \pm \infty$ $\forall_i$.
		Regarding your plots,
		\begin{itemize}
			\item $S_1(x_i(c_0)) = 0$ is not possible $\forall_i$ (ground state only approaches zero).
			\item $S_1(x_i(c_0)) \to - \infty$ is not possible $\forall_i$ (ground state $>0$).
			\item $S_1(x_i(c_0)) \to + \infty$ is possible $\forall_i$ if $c_0 \to 0^-$.
		\end{itemize}
	\item For $d = 2$ find $c_0$ such that $S_2(x_i(c_0)) \to + \infty$ $\forall_i$.
		Regarding your plots,
		\begin{itemize}
			\item $S_2(x_i(c_0)) \to + \infty$ is possible for $c_0 \to 0^-$.
		\end{itemize}
	\item For $d = 3$ find $c_0$ such that $S_3(x_i(c_0)) \to 0, \pm \infty$ $\forall_i$.
		Regarding your plots,
		\begin{itemize}
			\item $S_3(x_i(c_0)) = 0$ is possible $\forall_i$.
			\item $S_3(x_i(c_0)) \to \pm \infty$ is possible for $c_0 \to 0^\mp$.
		\end{itemize}
\end{itemize}

Obviously, the cases for $c_0 \to 0^\pm$ make sense --- if there is no interaction, than there is only the mass scale.
However this solution is not of interest.
This leaves $d=3$ as the only interesting case.

\newpage
\section{Momentum Space confusion}
This is a non complete description!
It should serve as a summary.

As we figured out most recently, it is interesting to look at Lüschers equation in a discrete finite volume if you want to connect discrete finite energy levels with continuum infinite volume phase shifts.
If you don't do so, you find that the discretization (in a finite volume) apparently induces an effective range (see figure \ref{fig-cont-lusch}).
\begin{figure*}[!htb]
\includegraphics[width=0.9\textwidth]{figs/eff-range-cont-lusch.pdf}
\caption{
    \label{fig-cont-lusch}Phase shifts extracted from continuum Lüscher equation for discrete finite volume eigenvalues of contact interaction Hamiltonian.
    The contact interactions was chosen such that the ground state matches the first zero of the zeta function.
    $\epsilon$ is the lattice spacing in [fm] for a box of size $L = 1$ [fm].
    $n_{\mathrm{step}}$ describes the implementation of the Laplace derivative.
    E.g., $n_{\mathrm{step}} = 1$ corresponds to an one step derivative where corrections are expected to scale with $\epsilon^2$.
    The variable $x$ is directly proportional to the eigenvalues of the Hamiltonian $x = 2 \mu E L^2 / (2 \pi)^2$.
    Even thought the interaction has no effective range (contact interaction), the effective range expansion for more precise implementations of the derivative, certainly is proportional to $x$.
 }
\end{figure*}

The trick you have used to get there was to replace the finite volume continuum Zeta function .
\begin{equation}
    S_3(x) = \sum\limits_{\vec n \in M(\Lambda)} \frac{1}{\vec n ^2 - x} - 4 \pi \Lambda
    \, , \qquad
    M(\Lambda) = \left\{ \vec n \in \mathbb N \middle\vert 0 \leq n_i \leq \Lambda \right\}
\end{equation}
with a "discretized version"
\begin{equation}
    \mapsto
    S_3^{\mathrm{lat}}(x) = \frac{4 \pi^2}{L^2} \sum\limits_{\vec k \in M^\epsilon(L)} \frac{1}{\vec k ^2 - 2 \mu E} - \mathcal L \pi^2 \frac{L}{\epsilon}
    \, , \qquad
    M^\epsilon(L) = \left\{ \vec k \in \frac{2 \pi}{L} \mathbb N \middle\vert - \frac{\pi}{\epsilon} \leq k_i < \frac{\pi}{\epsilon} \right\}
    \, ,
\end{equation}
where $\mathcal L = 0.777551 \cdots$ is the normalization of the momentum integral and the sum is limited but the momentum cutoff of the discrete spatial lattice.

This certainly improved the effective range expansion (see fig.~\ref{fig-cont-lusch}).
\begin{figure*}[!htb]
\includegraphics[width=0.9\textwidth]{figs/eff-range-discrete-lusch.pdf}
\caption{
    \label{fig-cont-lusch}Phase shifts extracted from discrete Lüscher equation for discrete finite volume eigenvalues of contact interaction Hamiltonian.
    $\epsilon$ is the lattice spacing in [fm] for a box of size $L = 1$ [fm].
 }
\end{figure*}

Even thought the utilization of the discrete Lüscher equation improves the description, e.g., reduces the size of the induced effective range, the results are not perfectly constant (e.g., equal to zero because of the choice of the contact interaction).
On top of that, we now see curvature in the effective range expansion.
This might be still the cause of further discretization effects.
Next, I describe how one can improve on that.

\subsection{Removing all discretization effects -- making ERE flat}
\textbf{The idea:} Implement an exact discrete derivative by transforming to momentum space, multiplying by the exact $p^2$, and transforming back to coordinate space.
Doing so, remove all derivative discretization approximations and hopefully recover a flat effective range expansion.

\textbf{The conclusion:} This is not possible.

\textbf{The next step:} Implementing the "actual" discrete Lüscher equation.

\subsubsection{The idea}
The idea is straight forward:

\begin{equation}
	\braket{ \vec r ' | \hat H_0 | \vec r}
	=
	\int d \vec q^3 \int d \vec k^3 \braket{ \vec k | \hat H_0 | \vec q} \braket{\vec r' | \vec k} \braket{\vec q | \vec r}
	=
	\int d \vec q^3 H_0(\vec q) \exp\left\{ i \vec q \cdot \left( \vec r - \vec r '\right)\right\}
\end{equation}
Discrete FFTs are already implemented in python, so the only remaining challenge is to multiply the transformed input times the expectation values of the kinetic Hamiltionian $H_0(\vec q)$.

But this turned out to be a major source of confusion.
As an example, suppose you have four spatial sites in a one dimensional discrete box with periodic boundary conditions.
This means, your momenta are discrete $\ket{\vec p} = \ket{2 \pi / L \vec n}$ with $\vec n \in \mathbb Z^3$ and periodic  $\ket{\vec p + 2 \pi / \epsilon} = \ket{\vec p}$.
A possible choice for momentum basis states are
\begin{equation}
	B_1(L = 4\epsilon, \epsilon)
	= \left\{ \frac{2\pi}{4\epsilon} \times n \middle \vert n \in \{-2, -1, 0, 1\} \right\}
	= \left\{ -\frac{\pi}{\epsilon}, -\frac{\pi}{2\epsilon}, 0, \frac{\pi}{2\epsilon} \right\} \, .
\end{equation}
The naive choice of eigenvalues are thus
\begin{equation}
    H(B_1) = \frac{1}{2\mu} \frac{\pi^2}{4 \epsilon^2} \left\{ 4, 1, 0, 1\right\} \, .
\end{equation}
But because of the periodicity of momenta $\ket{\vec p} = \ket{\vec p + 2\pi/\epsilon}$, the following basis is a valid choice as well:
\begin{equation}
	B_2(L = 4\epsilon, \epsilon)
	= \left\{ \frac{2\pi}{4\epsilon} \times n \middle \vert n \in \{0, 1, 2, 3\} \right\}
	= \left\{ 0, \frac{\pi}{2\epsilon}, \frac{\pi}{\epsilon}, \frac{3\pi}{2\epsilon} \right\} \, , \qquad
	H(B_2) = \frac{1}{2\mu} \frac{\pi^2}{4 \epsilon^2} \left\{ 0, 1, 4, 9\right\} \, .
\end{equation}
And since the discrete Fourier transformations are unitary transformations (and thus the eigenvalues of the Hamiltonian do not change through FTs), clearly the different implementations of the Hamiltonian return different answers.

Most interestingly, when actually implementing a 1-step derivative and running the FFTs in python, the first basis is actually preferred.

What's happening?

You have to manually take care of the periodicity!
Look at coordinate space.
Lüscher actually defines his finite volume periodic boundaries operators such that
\begin{equation}
    O_L(\vec r) = O_L(\vec r + \vec n L) = \frac{1}{\mathcal N} \sum _{\vec n \in \mathbb Z} O(\vec r + \vec n L)\, ,
\end{equation}
where $\mathcal N$ is a normalization and the infinite volume operator has a finite range such that $\lim\limits_{L\to \infty} O_L(\vec r) = O(\vec r)$.

So, how do we do the same thing for momenta.
The answer is basically the same.
Implement the expectation values of the operators such that
\begin{itemize}
    \item[(A)] The operators $O_\epsilon$ are periodic in $\frac{2\pi}{\epsilon}$ and
    \item[(B)] In the limit of $\epsilon \to 0$, you obtain  $O_\epsilon \to  O$.
\end{itemize}

So what's a valid choice?
\begin{equation}
	D^1( \vec p, \epsilon) = \sum_{i=1}^3 \frac{2 - 2 \cos(\epsilon p_i)}{\epsilon^2} \, .
\end{equation}
That's just the same thing you would do in coordinate space by introducing a finite step derivative.
In principal you can also do better and implement improved operators for faster convergence.

In other words: a naive infinite order improvement is not possible -- it does not matter where or how you implement your derivative.
If you take care of discretization and periodic boundary conditions, there is always a one to one correspondence of momentum and coordinate space.
At some point, it might be more efficient numerically though to run computations in one or the other space...

But here is the actual catch: since the momentum dispersion relation you have used $D^\infty(\vec p, \epsilon) = \vec p^2$ is actually not obtainable on a lattice (unless $n_{\mathrm{step}} \to \infty$), you should use the actual lattice dispersion relation (spectrum of $H_0$) to execute the Lüscher sum.

\subsection{New equations}

What I am suggesting is that the new equations are the following:
Instead of the infinitely improved dispersion relation $D^\infty(\vec p, \epsilon)$ (which is the continuum dispersion relation in a fixed interval), you should use the discrete lattice dispersion relation
\begin{equation}
    D^\infty(\vec p, \epsilon) = \vec p ^2 
    \to 
    D^{n_{\mathrm{s}}}(\vec p, \epsilon) 
    =
    \frac{1}{\epsilon^2} \sum\limits_{i=1}^3 \sum\limits_{n_i=-n_{\mathrm{s}}}^{n_{\mathrm{s}}} c_{|n_i|}^{n_{\mathrm{s}}} \cos( p_i n_i \epsilon)
\end{equation}
Where the coefficients $c_i^{n_{\mathrm{s}}}$ are enforced by the constrained
\begin{equation}
    D^{n_{\mathrm{s}}}(\vec p, \epsilon)
    =
    p^2 + \mathcal O \left(\epsilon^{2 n_{\mathrm{s}}}\right)
    \, .
\end{equation}
A few coefficients are given in table \ref{tab-dispersion-coeff}.

\begin{table}[htb]
\centering
\begin{tabular}{r | ccccc}
$ c_{i}^{n_{\mathrm{s}}}$ & i = 0 & 1 & 2 & 3 & 4 \\ \hline
$n_{\mathrm{s}} = 1$ & 2 & -1 \\
2 & $\frac{5}{2}$ & $-\frac{4}{3}$ & $\frac{1}{12}$ \\
3 & $\frac{49}{18}$ & $-\frac{3}{2}$ & $\frac{3}{20}$ & $-\frac{1}{90}$ \\
4 & $\frac{205}{72}$ & $-\frac{8}{5}$ & $\frac{1}{5}$  & $-\frac{8}{315}$ & $\frac{1}{560}$
\end{tabular}
\caption{\label{tab-dispersion-coeff}Coefficients for discrete momentum dispersion relation.
    Note that this coefficients are minus the Laplace finite step coefficients (because $\partial_x^2 \leftrightarrow - p^2$).
}
\end{table}

Thus one has to update the counter term in the zeta function definition (your $m = 2\mu$) (do I get the factors right)
\begin{equation}
    \frac{\pi}{\epsilon}
    \mathcal L
    =
    \frac{4 \pi}{(2\pi)^3}
    \left(
    \prod \limits_{i=1}^3
    \int\limits_{-\pi /\epsilon}^{\pi / \epsilon}
    d q_i
    \right)
    \frac{1}{q_1^2 + q_2^2 + q_3^2}
    \to
    \frac{4 \pi}{(2\pi)^3}
    \left(
    \prod \limits_{i=1}^3
    \int\limits_{-\pi / \epsilon}^{\pi / \epsilon}
    d q_i
    \right)
    \frac{1}{D^{n_{\mathrm{s}}}(\vec p, \epsilon)}
\end{equation}
and adjust the zeta function
\begin{equation}
    S_3^{\mathrm{lat}}(x) = \frac{4 \pi^2}{L^2} \sum\limits_{\vec k \in M^\epsilon(L)} \frac{1}{\vec D^{n_{\mathrm{s}}}(\vec p, \epsilon) - 2 \mu E} - \mathcal L \pi^2 \frac{L}{\epsilon}
    \, , \qquad
    M^\epsilon(L) = \left\{ \vec k \in \frac{2 \pi}{L} \mathbb N \middle\vert - \frac{\pi}{\epsilon} \leq k_i < \frac{\pi}{\epsilon} \right\}
    \, ,
\end{equation}

Let me know what you think about that.

\newpage
\section{Long Range Forces}
Let's consider long-range forces.
An analytically solvable potential is the best starting point to crosscheck lattice results in a finite volume.
The potential of choice has the following form
\begin{align}
	V(\vec p', \vec p) &= - g^*(\vec p') g(\vec p) \, ,
	&
	g(\vec{p})=\overline{g} \sqrt{8 \pi} \frac{M^{3}}{\left(\vec{p}^{2}+M^{2}\right)^{2}}, \quad M, \overline{g} \in \mathbb{R}
	\, .
\end{align}
For this choice, the two-nucleon binding momentum $\gamma = \sqrt{-2 \mu E}$ (where $E < 0$ is the ground state energy and $\mu$ the reduced mass of the two-nucleon system) is related to the potential parameters by
\begin{equation}
	\overline{g} = \frac{2M(\gamma+M)^{2}}{\sqrt{\mu M^{3}\left(\gamma^{2}+5 M^{2}+4 \gamma M\right)}} \, .
\end{equation}

To compute the phase shifts, one must know the $T$-matrix which is given by
\begin{align}
	T(\vec p', \vec p, E)
	&=
	V(\vec p'. \vec p) + \lim\limits_{\epsilon \to 0}\int \frac{d \vec k^3}{(2\pi)^3} V(\vec p', \vec k) G(\vec k, E + i \epsilon) T(\vec k, \vec p, E) \, ,
	&
	G(\vec k, E+ i \epsilon) = \frac{1}{E + i \epsilon - \frac{k^2}{2\mu}}
	\, .
\end{align}
Because of the separable potential, the equation factorizes to
\begin{align}
	T(\vec p', \vec p, E)
	&=
	- g^*(\vec p') \left[ g(\vec p) + \Gamma(\vec p, E) \right] \, ,
	&
	\Gamma(\vec p, E)
	&\equiv
	\lim\limits_{\epsilon \to 0}\int \frac{d \vec k^3}{(2\pi)^3} g(\vec k) G(\vec k, E + i \epsilon) T(\vec k, \vec p, E)
	\, .
\end{align}
Substituting $T$ back into the definition of $\Gamma$, one finds
\begin{align}
	\Gamma(\vec p, E)
	&= - \left[ g(\vec p) + \Gamma(\vec p, E) \right] I_0(E)
	\, , &
	I_0(E) &= \lim\limits_{\epsilon \to 0}\int \frac{d \vec k^3}{(2\pi)^3} g(\vec k) G(\vec k, E + i \epsilon) g^*(\vec k) 
	\\&&&= - \lim\limits_{\epsilon \to 0}\int \frac{d \vec k^3}{(2\pi)^3}  G(\vec k, E + i \epsilon) V(\vec k, \vec k)
	\, ,
\end{align}
or equivalently
\begin{equation}
	\Gamma(\vec p, E) = - \frac{I_0(E)}{1 + I_0(E)}g(\vec p) \, .
\end{equation}
Thus the $T$-matrix becomes
\begin{equation}
	T(\vec p', \vec p, E)
	=
	\frac{V(\vec p', \vec p)}{1 + I_0(E)}
	\, .
\end{equation}
The result for the integral is
\begin{equation}
	I_0\left(E = - \frac{\gamma^2}{2\mu}\right)
	=
	- |\bar g|^2 \mu  M \frac{\gamma ^2+5 M^2+4 \gamma  M}{4(\gamma +M)^4}
	\, .
\end{equation}
If one chooses $\bar g$ as above, one can verify that indeed $I_0(E = - \gamma^2/(2\mu)) = -1$ and thus forms the pole of the $T$-matrix.

In general, one has
\begin{equation}
	I_0\left(E = \frac{k^2}{2\mu}\right)
	=
	-|\bar g|^2 \mu  M \frac{
		-k^6-5 k^4 M^2+M^5 \left(5 M+16 i \sqrt{k^2}\right)-15 k^2 M^4 \sqrt{k^2}
	}{
		4\left(k^2+M^2\right)^4
	}
\end{equation}

And finally using 
\begin{equation}
	\frac{2 \pi}{ \mu k } \frac{1}{\cot(\delta_0(k)) - i} = - T_0\left(\vec k, \vec k, \frac{k^2}{2 \mu} \right) 
\end{equation}
\begin{equation}
	k \cot( \delta_0(k))
	=
	-\frac{5 |\bar g|^2 \mu  M-4 M^2}{16 |\bar g|^2 \mu }
	+\frac{ \left(15 |\bar g|^2 \mu + 16 M\right)}{16 |\bar g|^2 \mu M}k^2
	+\frac{ \left(5 |\bar g|^2 \mu + 24 M\right)}{16 |\bar g|^2\mu  M^3}k^4
	+\frac{ \left(|\bar g|^2 \mu + 16 M\right)}{16|\bar g|^2 \mu  M^5}k^6
	+\frac{1}{4 |\bar g|^2 \mu  M^6}k^8
\end{equation}

To bring the test close to what we would expect, let's choose
\begin{itemize}
	\item $\mu = \frac{m_N}{2} = 469.46 \MeV = 4.758 \ifm$
	\item $E_D = -2.225 \MeV = 0.0113 \fm$
	\item $\gamma = \sqrt{- 2 \mu E_D} = 45.707 \MeV = 0.2316 \ifm$
	\item $a_0 = 0.12 \MeV^{-1} = 23.5 \fm$
	\item $\Rightarrow \bar g = 0.8945$ and $ M = 28.383 \MeV= 0.1438 \ifm$
\end{itemize}
The coordinate space potential scales as $V(\vec r', \vec r) \sim \exp \left\{ - M (r + r')\right\}$. Thus one must choose $M L > 4$ which would bring us to $L \sim 28 \fm$...


\end{document}